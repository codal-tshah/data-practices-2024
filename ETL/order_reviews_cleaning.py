# -*- coding: utf-8 -*-
"""order_reviews_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DUR4B3Vqs5kYorxdEQReEAvc1RyduBnS
"""

import pandas as pd

"""reading the data and skipping the data where bad lines are there"""

df = pd.read_csv(
    "/content/drive/MyDrive/data_practices/order_reviews.csv", on_bad_lines="skip"
)

df.head()

"""couting total rows for each columns"""

df.count()

"""data structure information"""

print("shape", df.shape)
print("info", df.info())

df.describe()

df

"""counting total null values in each column"""

df.isnull().sum()

"""Removed two columns which is not necessary"""

df1 = df.drop(["review_comment_title", "review_comment_message"], axis=1)
df1

"""dropping rows having null values in order_id column


"""

droping_order_id = df1.dropna(subset=["order_id"], axis=0)
droping_order_id

print(droping_order_id.shape)
print(droping_order_id.isnull().sum())

# print("describe", droping_order_id.describe())
droping_order_id.describe()

df1 = droping_order_id
df1.head(10)

"""making proper datatypes of colums

"""

df1.dtypes

"""clean the columns for removing date from time stamp and removing timestamp from date column

We can use both strip and split
"""

df1["review_creation_date"] = df1["review_creation_date"].str.split(" ").str[0]
df1.head()

df1["review_answer_timestamp"] = df1["review_answer_timestamp"].str.split(" ").str[1]
df1.head()

df1["order_id"].unique()

print(df1.review_score.unique())

"""removing the review_score data which are not in range of 1 to 5  """

# df1['review_score'].unique().exclude('1','2','3','4','5')
temp_data = df1[df1["review_score"].isin({"1", "2", "3", "4", "5"})]
temp_data.head()

"""temp_data removed 4200 rows which dont contain review_score values in range"""

temp_data

temp_data["review_score"].unique()
temp_data["review_score"].shape
temp_data["review_score"].describe()
temp_data["review_score"].isnull().sum()

"""storing back temp_data to df1 as review_score which removed 4200 rows got NaN values and stored back

"""

df1["review_score"] = temp_data["review_score"]
df1.tail(10)

df1

df1.value_counts

"""to see which are duplicates data in review_id"""

df1.duplicated(subset=["review_id"])

duplicate = df1[df1.duplicated(subset=["review_id"])]
duplicate

duplicate.shape
duplicate.describe()

"""to remove duplicates data from review_id"""

df2 = df1.drop_duplicates(subset=["review_id"])
print(df2)

"""dropping the values which is null for all columns"""

# This is the correct way to remove the hexadecimal 32 character string

# # Define the regex pattern for a 32-character hexadecimal string
# hex_pattern = re.compile(r'^[0-9a-fA-F]{32}$')

# # Filter rows based on whether 'review_id' matches the hex pattern
# df_cleaned = df[df['review_id'].apply(lambda x: bool(hex_pattern.match(str(x))))]

df3 = df2[df2["review_id"].str.len() == 32]
print(df3)

df3.isnull().sum()

df3 = df3.dropna(subset=["review_score"], axis=0)
print(df3)

df3.isnull().sum()

df3.dtypes


"""conversion of columns datatypes"""

# Convert 'review_score' to integer
df3["review_score"] = df3["review_score"].astype(int)

df4 = df3

# Convert 'review_creation_date' to datetime, coerce errors to NaT
df4["review_creation_date"] = pd.to_datetime(
    df3["review_creation_date"], errors="coerce"
)

# Convert 'review_answer_timestamp' to datetime, coerce errors to NaT
df4["review_answer_timestamp"] = pd.to_datetime(
    df3["review_answer_timestamp"], format="%H:%M:%S", errors="coerce"
).dt.time

# Drop rows where 'review_creation_date' or 'review_answer_timestamp' is NaT
df_cleaned = df4.dropna(subset=["review_creation_date", "review_answer_timestamp"])

# Display the cleaned DataFrame
print(df_cleaned)

df_cleaned.dropna()

print(df_cleaned.isnull().sum())

print("shape of dataframe", df_cleaned.shape)

df_cleaned.dtypes

df_cleaned.to_csv("cleaned_order_reviews.csv", index=False, encoding="utf-8")
